{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Benchmarking and Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling** is the technique that allows to pinpoint the most resource-intensive spots in an application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiler** is a program that runs an application and monitors how long each function takes to execute, thus detecting the functions in which application spends most of its time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmarks** are small scripts used to assess the total execution time of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Designing your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Make it run**: We have to get the software in a working state, and ensure that it produces the correct results. This exploratory phase serves to better understand the application and to spot major design issues in the early stages.\n",
    "2. **Make it right**: We want to ensure that the design of the program is solid. Refactoring should be done before attempting any performance optimization. This really helps separate the application into independent and cohesive units that are easier to maintain.\n",
    "3. **Make it fast**: Once our program is working and is well structured, we can focus on performance optimization. We may also want to optimize memory usage if that constitutes an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the particle position at time t**\n",
    "1. Calculate the direction of motion ( v_x and v_y).\n",
    "2. Calculate the displacement (d_x and d_y), which is the product of time step, angular velocity, and direction of motion.\n",
    "3. Repeat steps 1 and 2 for enough times to cover the total time t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize particle trajectory using matplotlib**\n",
    "1. Set up the axes and use the plot function to display the particles. plot takes a list of x and y coordinates.\n",
    "2. Write an initialization function, init, and a function, animate, that updates the x and y coordinates using the line.set_data method\n",
    "3. Create a FuncAnimation instance by passing the init and animate functions plus the interval parameters, which specify the update interval, and blit, which improves the update rate of the image\n",
    "4. Run the animation with plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *test_visualize* function is helpful to graphically understand the system time\n",
    "evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Writing tests and benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A benchmark is a simple and representative use case that can be run to assess the running time of an application. Benchmarks are very useful to keep score of how fast the program is with each new version that implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timing the benchmark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, *time* displays three metrics:\n",
    "1. **real**: The actual time spent running the process from start to finish, as if it was measured by a human with a stopwatch\n",
    "2. **user**: The cumulative time spent by all the CPUs during the computation\n",
    "3. **sys**: The cumulative time spent by all the CPUs during system-related tasks, such as memory allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***simul.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from random import uniform\n",
    "import timeit\n",
    "\n",
    "class Particle:\n",
    "\n",
    "    __slots__ = ('x', 'y', 'ang_speed')\n",
    "\n",
    "    def __init__(self, x, y, ang_speed):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.ang_speed = ang_speed\n",
    "\n",
    "\n",
    "class ParticleSimulator:\n",
    "\n",
    "    def __init__(self, particles):\n",
    "        self.particles = particles\n",
    "\n",
    "    def evolve(self, dt):\n",
    "        timestep = 0.00001\n",
    "        nsteps = int(dt/timestep)\n",
    "\n",
    "        for i in range(nsteps):\n",
    "            for p in self.particles:\n",
    "                # 1. calculate the direction\n",
    "                norm = (p.x**2 + p.y**2)**0.5\n",
    "                v_x = (-p.y)/norm\n",
    "                v_y = p.x/norm\n",
    "                \n",
    "                # 2. calculate the displacement\n",
    "                d_x = timestep * p.ang_speed * v_x\n",
    "                d_y = timestep * p.ang_speed * v_y\n",
    "\n",
    "                p.x += d_x\n",
    "                p.y += d_y\n",
    "                # 3. repeat for all the time steps\n",
    "\n",
    "    # def evolve(self, dt):\n",
    "    #     timestep = 0.00001\n",
    "    #     nsteps = int(dt/timestep)\n",
    "\n",
    "    #     # First, change the loop order\n",
    "    #     for p in self.particles:\n",
    "    #         t_x_ang = timestep * p.ang_speed\n",
    "    #         for i in range(nsteps):\n",
    "    #             norm = (p.x**2 + p.y**2)**0.5\n",
    "    #             p.x, p.y = p.x - t_x_ang*p.y/norm, p.y + t_x_ang * p.x/norm\n",
    "\n",
    "def visualize(simulator):\n",
    "\n",
    "    X = [p.x for p in simulator.particles]\n",
    "    Y = [p.y for p in simulator.particles]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111, aspect='equal')\n",
    "    line, = ax.plot(X, Y, 'ro')\n",
    "\n",
    "    # Axis limits\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "\n",
    "    # It will be run when the animation starts\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        return line,\n",
    "\n",
    "    def animate(i):\n",
    "        # We let the particle evolve for 0.1 time units\n",
    "        simulator.evolve(0.01)\n",
    "        X = [p.x for p in simulator.particles]\n",
    "        Y = [p.y for p in simulator.particles]\n",
    "\n",
    "        line.set_data(X, Y)\n",
    "        return line,\n",
    "\n",
    "    # Call the animate function each 10 ms\n",
    "    anim = animation.FuncAnimation(fig,\n",
    "                                   animate,\n",
    "                                   init_func=init,\n",
    "                                   blit=True,\n",
    "                                   interval=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_visualize():\n",
    "    particles = [Particle( 0.3, 0.5, +1),\n",
    "                 Particle( 0.0, -0.5, -1),\n",
    "                 Particle(-0.1, -0.4, +3)]\n",
    "\n",
    "    simulator = ParticleSimulator(particles)\n",
    "    visualize(simulator)\n",
    "\n",
    "def test_evolve():\n",
    "    particles = [Particle( 0.3,  0.5, +1),\n",
    "                 Particle( 0.0, -0.5, -1),\n",
    "                 Particle(-0.1, -0.4, +3)]\n",
    "\n",
    "    simulator = ParticleSimulator(particles)\n",
    "\n",
    "    simulator.evolve(0.1)\n",
    "\n",
    "    p0, p1, p2 = particles\n",
    "\n",
    "    def fequal(a, b):\n",
    "        return abs(a - b) < 1e-5\n",
    "\n",
    "    assert fequal(p0.x, 0.2102698450356825)\n",
    "    assert fequal(p0.y, 0.5438635787296997)\n",
    "\n",
    "    assert fequal(p1.x, -0.0993347660567358)\n",
    "    assert fequal(p1.y, -0.4900342888538049)\n",
    "\n",
    "    assert fequal(p2.x,  0.1913585038252641)\n",
    "    assert fequal(p2.y, -0.3652272210744360)\n",
    "\n",
    "\n",
    "def benchmark():\n",
    "    particles = [Particle(uniform(-1.0, 1.0),\n",
    "                          uniform(-1.0, 1.0),\n",
    "                          uniform(-1.0, 1.0))\n",
    "                  for i in range(100)]\n",
    "\n",
    "    simulator = ParticleSimulator(particles)\n",
    "    simulator.evolve(0.1)\n",
    "\n",
    "\n",
    "def timing():\n",
    "    result = timeit.timeit('benchmark()',\n",
    "                           setup='from __main__ import benchmark',\n",
    "                           number=10)\n",
    "    # Result is the time it takes to run the whole loop\n",
    "    print(result)\n",
    "\n",
    "    result = timeit.repeat('benchmark()',\n",
    "                           setup='from __main__ import benchmark',\n",
    "                           number=10,\n",
    "                           repeat=3)\n",
    "    # Result is a list of times\n",
    "    print(result)\n",
    "\n",
    "\n",
    "def benchmark_memory():\n",
    "    particles = [Particle(uniform(-1.0, 1.0),\n",
    "                          uniform(-1.0, 1.0),\n",
    "                          uniform(-1.0, 1.0))\n",
    "                  for i in range(100000)]\n",
    "\n",
    "    simulator = ParticleSimulator(particles)\n",
    "    simulator.evolve(0.001)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Better tests and benchmarks with pytest-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytest-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***test_simul.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simul import Particle, ParticleSimulator\n",
    "\n",
    "def test_evolve(benchmark):\n",
    "    particles = [Particle( 0.3,  0.5, +1),\n",
    "                 Particle( 0.0, -0.5, -1),\n",
    "                 Particle(-0.1, -0.4, +3)]\n",
    "\n",
    "    simulator = ParticleSimulator(particles)\n",
    "\n",
    "    simulator.evolve(0.1)\n",
    "\n",
    "    p0, p1, p2 = particles\n",
    "\n",
    "    def fequal(a, b):\n",
    "        return abs(a - b) < 1e-5\n",
    "\n",
    "    assert fequal(p0.x, 0.2102698450356825)\n",
    "    assert fequal(p0.y, 0.5438635787296997)\n",
    "\n",
    "    assert fequal(p1.x, -0.0993347660567358)\n",
    "    assert fequal(p1.y, -0.4900342888538049)\n",
    "\n",
    "    assert fequal(p2.x,  0.1913585038252641)\n",
    "    assert fequal(p2.y, -0.3652272210744360)\n",
    "\n",
    "    benchmark(simulator.evolve, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_simul.py::test_evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Finding bottlenecks with cProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two profiling modules are available through the Python standard library:\n",
    "1. **profile**: This module is written in pure Python and adds a significant overhead to the program execution. Its presence in the standard library is because of its vast platform support and because it is easier to extend.\n",
    "2. **cProfile**: This is the main profiling module, with an interface equivalent to profile. It is written in C, has a small overhead, and is suitable as a general purpose profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cProfile module can be used in three different ways:\n",
    "1. From the command line\n",
    "2. As a Python module\n",
    "3. With IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m cProfile simul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m cProfile -s tottime simul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m cProfile -o prof.out simul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simul import benchmark\n",
    "import cProfile\n",
    "cProfile.run(\"benchmark()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simul import benchmark\n",
    "import cProfile\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "benchmark()\n",
    "pr.disable()\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simul import benchmark\n",
    "%prun benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cProfile output is divided into five columns:\n",
    "1. ncalls: The number of times the function was called.\n",
    "2. tottime: The total time spent in the function without taking into account the calls to other functions.\n",
    "3. cumtime: The time in the function including other function calls.\n",
    "4. percall: The time spent for a single call of the function--it can be obtained by dividing the total or cumulative time by the number of calls.\n",
    "5. filename:lineno: The filename and corresponding line numbers. This information is not available when calling C extensions modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important metric is tottime, the actual time spent in the function body excluding subcalls, which tell exactly where the bottleneck is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cProfile only provides information at the function level and does not tell us which specific statements are responsible for the bottleneck. Fortunately, the line_profiler tool is capable of providing line-by-line information of the time spent in the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KCachegrind** is a Graphical User Interface (GUI) useful to analyze the profiling output emitted by cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pyprof2calltree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***taylor.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n): \n",
    "    if n == 0: \n",
    "        return 1.0 \n",
    "    else: \n",
    "        return float(n) * factorial(n-1) \n",
    "\n",
    "def taylor_exp(n): \n",
    "    return [1.0/factorial(i) for i in range(n)] \n",
    "\n",
    "def taylor_sin(n): \n",
    "    res = [] \n",
    "    for i in range(n): \n",
    "        if i % 2 == 1: \n",
    "           res.append((-1)**((i-1)/2)/float(factorial(i))) \n",
    "        else: \n",
    "           res.append(0.0) \n",
    "    return res \n",
    "\n",
    "def benchmark(): \n",
    "    taylor_exp(500) \n",
    "    taylor_sin(500) \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    benchmark() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m cProfile -o prof.out taylor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyprof2calltree -i prof.out -o prof.calltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kcachegrind prof.calltree # or qcachegrind prof.calltree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mac users can compile QCacheGrind using Mac Ports (http://www.macports.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!QCacheGrind prof.calltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gprof2dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Profile line by line with line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which function we have to optimize, we can use the line_profiler module that provides information on how time is spent in a line-by-line fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use line_profiler, we need to apply a @profile decorator to the functions we intend to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def evolve(self, dt):\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernprof.py script will produce an output file and will print the result of the profiling on the standard output. We should run the script with two options:\n",
    "1. -l to use the line_profiler function\n",
    "2. -v to immediately print the results on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kernprof.py -l -v simul.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to run the profiler in an IPython shell for interactive editing. You should first load the line_profiler extension that will provide the lprun magic command. Using that command, you can avoid adding the @profile decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simul import benchmark, ParticleSimulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f ParticleSimulator.evolve benchmark() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Optimizing our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the algorithms used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = r * cos(alpha)\n",
    "y = r * sin(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the performance of the loop by reducing the number of assignment operations performed. To do that, we can avoid intermediate variables by rewriting the expression into a single, slightly more complex statement (note that the right-hand side gets evaluated completely before being assigned to the variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_fast(self, dt):\n",
    "    timestep = 0.00001\n",
    "    nsteps = int(dt/timestep)\n",
    "# Loop order is changed\n",
    "    for p in self.particles:\n",
    "        t_x_ang = timestep * p.ang_vel\n",
    "        for i in range(nsteps):\n",
    "            norm = (p.x**2 + p.y**2)**0.5\n",
    "            p.x, p.y = (p.x - t_x_ang * p.y/norm,\n",
    "                        p.y + t_x_ang * p.x/norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time python simul.py # Performance Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time python simul.py # Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 The dis module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CPython interpreter, Python code is first converted to an intermediate representation, the bytecode, and then executed by the Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect how the code is converted to bytecode, we can use the dis Python module (dis stands for disassemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from simul import ParticleSimulator\n",
    "dis.dis(ParticleSimulator.evolve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dis module helps discover how the statements get converted and serves mainly as an exploration and learning tool of the Python bytecode representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Profiling memory usage with memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory_profiler module summarizes, in a way similar to line_profiler, the memory usage of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like line_profiler, memory_profiler also requires the instrumentation of the source code by placing a @profile decorator on the function we intend to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memory():\n",
    "    particles = [Particle(uniform(-1.0, 1.0),\n",
    "                          uniform(-1.0, 1.0),\n",
    "                          uniform(-1.0, 1.0))\n",
    "                    for i in range(100000)]\n",
    "    simulator = ParticleSimulator(particles)\n",
    "    simulator.evolve(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use memory_profiler from an IPython shell through the %mprun magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simul import benchmark_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f benchmark_memory benchmark_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    __slots__ = ('x', 'y', 'ang_vel')\n",
    "def __init__(self, x, y, ang_vel):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    self.ang_vel = ang_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
