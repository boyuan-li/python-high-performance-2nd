{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Pure Python Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buying a larger server or microoptimizing can work for some time, but achieving better scaling through algorithmic improvement can solve the problem once and for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 2.1 Useful algorithms and data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm running times can be classified according to their computational complexity, a characterization of the resources required to perform a task. Such classification is expressed through the Big-O notation, an upper bound on the operations required to execute the task, which usually depends on the input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, incrementing each element of a list can be implemented using a for loop, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = list(range(10))\n",
    "for i, _ in enumerate(input):\n",
    "        input[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the operation does not depend on the size of the input (for example, accessing the first element of a list), the algorithm is said to take constant, or O(1), time. This means that, no matter how much data we have, the time to run the algorithm will always be the same. In this simple algorithm, the input[i] += 1 operation will be repeated 10 times, which is the size of the input. If we double the size of the input array, the number of operations will increase proportionally. Since the number of operations is proportional to the input size, this algorithm is said to take O(N) time, where N is the size of the input array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists and deques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists are ordered collections of elements and, in Python, are implemented as resizable arrays. An array is a basic data structure that consists of a series of contiguous memory locations, and each location contains a reference to a Python object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists shine in accessing, modifying, and appending elements. Accessing or modifying an element involves fetching the object reference from the appropriate position of the underlying array and has complexity O(1). Appending an element is also very fast. When an empty list is created, an array of fixed size is allocated and, as we insert elements, the slots in the array are gradually filled up. Once all the slots are occupied, the list needs to increase the size of its underlying array, thus triggering a memory reallocation that can take O(N) time. Nevertheless, those memory allocations are infrequent, and the time complexity for the append operation is referred to as amortized O(1) time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list operations that may have efficiency problems are those that add or remove elements at the beginning (or somewhere in the middle) of the list. When an item is inserted, or removed, from the beginning of a list, all the subsequent elements of the array need to be shifted by a position, thus taking O(N) time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*list.pop(), O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*list.pop(0), O(N)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*list.append(1), O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*list.insert(0,1), O(N)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, it is necessary to efficiently perform insertion or removal of elements both at the beginning and at the end of the collection. Python provides a data structure with those properties in the collections.deque class. The word deque stands for double-ended queue because this data structure is designed to efficiently put and remove elements at the beginning and at the end of the collection, as it is in the case of queues. In Python, deques are implemented as doubly-linked lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*deque.pop(), O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*deque.popleft(), O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*deque.append(1), O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*deque.appendleft(1), O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*deque[int(N/2)], O(N)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for an item in a list is generally a O(N) operation and is performed using the list.index method. A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the bisect module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisect module allows fast searches on sorted arrays. The bisect.bisect function can be used on a sorted list to find the index to place an element while maintaining the array in sorted order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we can see that if we want to insert the 3 element in the array while keeping collection in sorted order, we should put 3 in the third position (which corresponds to index 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert bisect\n",
    "collection = [1, 2, 4, 5, 6]\n",
    "bisect.bisect(collection, 3)\n",
    "# Result: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses the binary search algorithm that has O(log(N)) running time. Such a running time is exceptionally fast, and basically means that your running time will increase by a constant amount every time you double your input size. This means that if, for example, your program takes 1 second to run on an input of size 1000, it will take 2 seconds to process an input of size 2000, 3 seconds to process an input of size 4000, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value we are trying to insert is already present in the list, the bisect.bisect function will return the location after the already present value. Therefore, we can use the bisect.bisect_left variant, which returns the correct index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_bisect(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect.bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "    return i\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*list.index(a), O(N)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*index_bisect(list, a), O(log(N))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries are implemented as hash maps and are very good at element insertion, deletion, and access; all these operations have an average O(1) time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python versions up to 3.5, dictionaries are unordered collections. Since Python 3.6, dictionaries are capable of maintaining their elements by order of insertion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **hash map** is a data structure that associates a set of **key-value pairs**. The principle behind hash maps is to assign a specific index to each key so that its associated value can be stored in an array. The index can be obtained through the use of a hash function; Python implements hash functions for several data types. As a demonstration, the generic function to obtain hash codes is hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(\"hello\")\n",
    "# Result: -1182655621190490452\n",
    "# To restrict the number to be a certain range you can use the modulo (%) operator\n",
    "hash(\"hello\") % 10\n",
    "# Result: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hash maps can be tricky to implement because they need to handle collisions that happen when two different objects have the same hash code. However, all the complexity is elegantly hidden behind the implementation and the default collision resolution works well in most real-world scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access, insertion, and removal of an item in a dictionary scales as O(1) with the size of the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary can be used to efficiently count unique elements in a list. In this example, we define the counter_dict function that takes a list and returns a dictionary containing the number of occurrences of each value in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*counter_dict(items), O(N)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_dict(items):\n",
    "    counter = {}\n",
    "    for item in items:\n",
    "        if item not in counter:\n",
    "            counter[item] = 0\n",
    "        else:\n",
    "            counter[item] += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, the defaultdict(int) call produces a dictionary where every new element is automatically assigned a zero value, and can be used to streamline the counting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*counter_defaultdict(items), O(N)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def counter_defaultdict(items):\n",
    "    counter = defaultdict(int)\n",
    "    for item in items:\n",
    "        counter[item] += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Counter(items), O(N)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an in-memory search index using a hash map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries can be used to quickly search for a word in a list of documents, similar to a search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to retrieve all the documents that match a query is to scan each document and test for the presence of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per-query cost of the linear scan is O(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the cat is under the table', 'the dog is under the table']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"the cat is under the table\",\n",
    "        \"the dog is under the table\",\n",
    "        \"cats and dogs smell roses\",\n",
    "        \"Carla eats an apple\"]\n",
    "\n",
    "matches = [doc for doc in docs if \"table\" in doc]\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a structure, called the inverted index, that associates each word in our collection with the list of documents where that word is present. In our earlier example, the word \"table\" will be associated to the \"the cat is under the table\" and \"the dog is under the table\" documents; they correspond to indices 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the cat is under the table', 'the dog is under the table']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building an index\n",
    "index = {}\n",
    "for i, doc in enumerate(docs):\n",
    "    # We iterate over each term in the document\n",
    "    for word in doc.split():\n",
    "        # We build a list containing the indices where the term appears\n",
    "        if word not in index:\n",
    "            index[word] = [i]\n",
    "        else:\n",
    "            index[word].append(i)\n",
    "            \n",
    "results = index['table']\n",
    "result_documents = [docs[i] for i in results]\n",
    "result_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all it takes to query our collection is a dictionary access, the index can handle queries with time complexity O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets are unordered collections of elements, with the additional restriction that the elements must be unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets are implemented using a hash-based algorithm just like dictionaries, the time complexities for addition, deletion, and test for membership scale as O(1) with the size of the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets used for duplicates removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list that contains duplicates\n",
    "x = list(range(1000)) + list(range(500))\n",
    "# the set *x_unique* will contain only the unique elements in x\n",
    "x_unique = set(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time complexity for removing duplicates is O(N), as it requires to read the input and put each element in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*s.union(t), O(S+T)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*s.intersection(t), O(min(S,T))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*s.difference(t), O(S)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building an index using sets\n",
    "index = {}\n",
    "for i, doc in enumerate(docs):\n",
    "    # We iterate over each term in the document\n",
    "    for word in doc.split():\n",
    "        # We build a set containing the indices where the term appears\n",
    "        if word not in index:\n",
    "            index[word] = {i}\n",
    "        else:\n",
    "            index[word].add(i)\n",
    "            \n",
    "# Querying the documents containing both \"cat\" and \"table\"\n",
    "index['cat'].intersection(index['table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heaps are data structures designed to quickly find and extract the maximum (or minimum) value in a collection. A typical use-case for heaps is to process a series of incoming tasks in order of maximum priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heap is a more efficient data structure that allows for insertion and extraction of maximum values with O(log(N)) time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "collection = [10, 3, 3, 4, 5, 6]\n",
    "heapq.heapify(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heapq.heappop(collection)\n",
    "# Returns: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq.heappush(collection, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3, 10, 5, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the minimum value in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "queue = PriorityQueue()\n",
    "for element in collection:\n",
    "    queue.put(element)\n",
    "    \n",
    "queue.get()\n",
    "# Returns: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'priority 1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue = PriorityQueue()\n",
    "queue.put((3, \"priority 3\"))\n",
    "queue.put((2, \"priority 2\"))\n",
    "queue.put((1, \"priority 1\"))\n",
    "queue.get()\n",
    "# Returns: (1, \"priority 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search-as-you type and autocompletion, where the list of available completions is very large and short response times are required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the task of finding the longest prefix in a set of strings (just like autocompletion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from string import ascii_uppercase\n",
    "\n",
    "def random_string(length):\n",
    "\"\"\"Produce a random string made of *length* uppercase ascii\n",
    "characters\"\"\"\n",
    "return ''.join(choice(ascii_uppercase) for i in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [random_string(32) for i in range(10000)]\n",
    "matches = [s for s in strings if s.startswith('AA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit [s for s in strings if s.startswith('AA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install patricia-trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patricia import trie\n",
    "strings_dict = {s:0 for s in strings}\n",
    "# A dictionary where all values are 0\n",
    "strings_trie = trie(**strings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = list(strings_trie.iter('AA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit list(strings_trie.iter('AA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying a trie has a time complexity O(S), where S is the length of the longest string in the collection, while the time complexity of a simple linear scan is O(N), where N is the size of the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 2.2 Caching and memoization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is a great technique used to improve the performance of a wide range of applications. The idea behind caching is to store expensive results in a temporary location, called cache, that can be located in memory, on-disk, or in a remote location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web applications make extensive use of caching. In a web application, it often happens that users request a certain page at the same time. In this case, instead of recomputing the page for each user, the web application can compute it once and serve the user the already rendered page. Ideally, caching also needs a mechanism for invalidation so that if the page needs to be updated, we can recompute it before serving it again. Intelligent caching allows web applications to handle increasing number of users with less resources. Caching can also be done preemptively, such as the later sections of the video get buffered when watching a video online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and reusing the results of the previous function calls in an application is usually termed as **memoization**, and is one of the forms of caching. Several other algorithms can take advantage of memoization to gain impressive performance improvements, and this programming technique is commonly referred to as **dynamic programming**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of caching, however, do not come for free. What we are actually doing is sacrificing some space to improve the speed of the application. Additionally, if the cache is stored in a location on the network, we may incur transfer costs and general time needed for communication. One should evaluate when it is convenient to use a cache and how much space we are willing to trade for an increase in speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python standard library includes a simple inmemory cache out of the box in the *functools* module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache()\n",
    "def sum2(a, b):\n",
    "    print(\"Calculating {} + {}\".format(a, b))\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 1 + 2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sum2(1, 2))\n",
    "# Output:\n",
    "# Calculating 1 + 2\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sum2(1, 2))\n",
    "# Output:\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(max_size=16)\n",
    "def sum2(a, b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum2.cache_info()\n",
    "# Output: CacheInfo(hits=0, misses=1, maxsize=128, currsize=1)\n",
    "sum2.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.88 ms ± 27.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "    \n",
    "# Non-memoized version\n",
    "%timeit fibonacci(20)\n",
    "# 100 loops, best of 3: 5.57 ms per loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previously computed fibonacci sequences are not reused, causing this algorithm to have an exponential scaling of roughly O(2N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching can improve this algorithm by storing and reusing the already-computed fibonacci numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibonacci took 0.00 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "setup_code = '''\n",
    "from functools import lru_cache\n",
    "from __main__ import fibonacci\n",
    "fibonacci_memoized = lru_cache(maxsize=None)(fibonacci)\n",
    "'''\n",
    "\n",
    "results = timeit.repeat('fibonacci_memoized(20)',\n",
    "                        setup=setup_code,\n",
    "                        repeat=1000,\n",
    "                        number=1)\n",
    "\n",
    "print(\"Fibonacci took {:.2f} us\".format(min(results)))\n",
    "# Output: Fibonacci took 0.01 us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib is on-disk cache. The package can be used in a similar way as lru_cache, except that the results will be stored on disk and will persist between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda update joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory(cachedir='/path/to/cachedir')\n",
    "\n",
    "@memory.cache\n",
    "def sum2(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will behave similar to lru_cache, with the exception that the results will be stored on-disk in the directory specified by the cachedir argument during Memory initialization. Additionally, the cached results will persist over subsequent runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the best joblib feature is that, thanks to intelligent hashing algorithms, it provides efficient memoization of functions that operate on numpy arrays, and is particularly useful in scientific and engineering applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 2.3 Comprehensions and generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.57 ms ± 88.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "5.64 ms ± 49.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "6.41 ms ± 94.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def loop():\n",
    "    res = []\n",
    "    for i in range(100000):\n",
    "        res.append(i * i)\n",
    "    return sum(res)\n",
    "\n",
    "def comprehension():\n",
    "    return sum([i * i for i in range(100000)])\n",
    "\n",
    "def generator():\n",
    "    return sum(i * i for i in range(100000))\n",
    "\n",
    "%timeit loop()\n",
    "# 100 loops, best of 3: 16.1 ms per loop\n",
    "%timeit comprehension()\n",
    "# 100 loops, best of 3: 10.1 ms per loop\n",
    "%timeit generator()\n",
    "# 100 loops, best of 3: 12.4 ms per loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.79 ms ± 169 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "5.03 ms ± 26 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def loop():\n",
    "    res = {}\n",
    "    for i in range(100000):\n",
    "        res[i] = i\n",
    "    return res\n",
    "\n",
    "def comprehension():\n",
    "    return {i: i for i in range(100000)}\n",
    "\n",
    "%timeit loop()\n",
    "# 100 loops, best of 3: 13.2 ms per loop\n",
    "%timeit comprehension()\n",
    "# 100 loops, best of 3: 12.8 ms per loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient looping (especially in terms of memory) can be implemented using iterators and functions such as filter and map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_comprehension(numbers):\n",
    "    a = [n * 2 for n in numbers]\n",
    "    b = [n ** 2 for n in a]\n",
    "    c = [n ** 0.33 for n in b]\n",
    "    return max(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is that for every list comprehension, we are allocating a new list, increasing memory usage. Instead of using list comprehension, we can employ generators. Generators are objects that, when iterated upon, compute a value on the fly and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the map function takes two arguments--a function and an iterator--and returns a generator that applies the function to every element of the collection. *The important point is that the operation happens only while we are iterating, and not when map is invoked!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the previous function using map and by creating intermediate generators, rather than lists, thus saving memory by computing the values on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_normal(numbers):\n",
    "    a = map(lambda n: n * 2, numbers)\n",
    "    b = map(lambda n: n ** 2, a)\n",
    "    c = map(lambda n: n ** 0.33, b)\n",
    "    return max(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "numbers = range(1000000)\n",
    "%memit map_comprehension(numbers)\n",
    "# peak memory: 166.33 MiB, increment: 102.54 MiB\n",
    "%memit map_normal(numbers)\n",
    "# peak memory: 71.04 MiB, increment: 0.00 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 2.4 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
