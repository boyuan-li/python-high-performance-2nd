{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. Implementing Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure and improve the performance of programs by reducing the number of operations performed by the CPU through clever algorithms and more efficient machine code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs where most of the time is spent waiting for resources that are much slower than the CPU, such as persistent storage and network resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous programming is a programming paradigm that helps to deal with slow and unpredictable resources (such as users) and is widely used to build responsive services and user interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 6.1 Asynchronous programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous programming is a way of dealing with slow and unpredictable resources. Rather than waiting idle for resources to become available, asynchronous programs are able to handle multiple resources concurrently and efficiently. Programming in an asynchronous way can be challenging because it is necessary to deal with external requests that can arrive in any order, may take a variable amount of time, or may fail unpredictably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Waiting for I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modern computer employs different kinds of memory to store data and perform operations. In general, a computer possesses a combination of expensive memory that is capable of operating at fast speeds and cheaper, and more abundant memory that operates at lower speeds and is used to store a larger amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registers**: At the top of the memory hierarchy are the CPU registers. Those are integrated in the CPU and are used to store and execute machine instructions. Accessing data in a register generally takes one clock cycle. This means that if the CPU operates at 3 GHz, the time it takes to access one element in a CPU register is in the order of 0.3 nanoseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache**: At the layer just below the registers, you can find the CPU cache, which is comprised of multiple levels and is integrated in the processor. The cache operates at a slightly slower speed than the registers but within the same order of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAM**: The next item in the hierarchy is the main memory (RAM), which holds much more data but is slower than the cache. Fetching an item from memory can take a few hundred clock cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storage**: At the bottom layer, you can find persistent storage, such as a rotating disks (HDD) and Solid State Drives (SSD). These devices hold the most data and are orders of magnitude slower than the main memory. An HDD may take a few milliseconds to seek and retrieve an item, while an SSD is substantially faster and takes only a fraction of a millisecond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put the relative speed of each memory type into perspective, if you were to have the CPU with a clock speed of about one second, a register access would be equivalent to picking up a pen from the table. A cache access will be equivalent to picking up a book from the shelf. Moving higher in the hierarchy, a RAM access will be equivalent to loading up the laundry (about twenty x slower than the cache). When we move to persistent storage, things are quite a bit different. Retrieving an element from an SSD will be equivalent to doing a four day trip, while retrieving an element from an HDD can take up to six months! The times can stretch even further if we move on to access resources over the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preceding example, it should be clear that accessing data from storage and other I/O devices is much slower compared to the CPU; therefore, it is very important to handle those resources so that the CPU is never stuck waiting aimlessly. This can be accomplished by carefully designing software capable of managing multiple, ongoing requests at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concurrency is a way to implement a system that is able to deal with multiple requests at the same time. The idea is that we can move on and start handling other resources while we wait for a resource to become available. Concurrency works by splitting a task into smaller subtasks that can be executed out of order so that multiple tasks can be partially advanced without waiting for the previous tasks to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first example, we will describe how to implement concurrent access to a slow network resource. Let's say we have a web service that takes the square of a number, and the time between our request and the response will be approximately one second. We can implement the network_request function that takes a number and returns a dictionary that contains information about the success of the operation and the result. We can simulate such services using the time.sleep function, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result is: 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def network_request(number):\n",
    "    time.sleep(1.0)\n",
    "    return {\"success\": True, \"result\": number ** 2}\n",
    "\n",
    "def fetch_square(number):\n",
    "    response = network_request(number)\n",
    "    if response[\"success\"]:\n",
    "        print(\"Result is: {}\".format(response[\"result\"]))\n",
    "        \n",
    "fetch_square(2)\n",
    "# Output:\n",
    "# Result is: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result is: 4\n",
      "Result is: 9\n",
      "Result is: 16\n"
     ]
    }
   ],
   "source": [
    "fetch_square(2)\n",
    "fetch_square(3)\n",
    "fetch_square(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code will take three seconds to run, but it's not the best we can do. Waiting for the previous result to finish is unnecessary as we can technically submit multiple requests at and wait for them parallely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you are at a restaurant and you've had a few drinks. It's raining outside, and you'd rather not take the bus; therefore, you request a taxi and ask them to call when they're outside so that you can come out, and you don't have to wait in the rain. What you did in this case is request a taxi (that is, the slow resource) but instead of waiting outside until the taxi arrives, you provide your number and instructions (callback) so that you can come outside when they're ready and go home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_and_print(msg):\n",
    "    time.sleep(1.0)\n",
    "    print(msg)\n",
    "    \n",
    "import threading\n",
    "\n",
    "def wait_and_print_async(msg):\n",
    "    def callback():\n",
    "        print(msg)\n",
    "        \n",
    "    timer = threading.Timer(1.0, callback)\n",
    "    timer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of the wait_and_print_async function is that none of the statements are blocking the execution flow of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique of registering callbacks for execution in response to certain events is commonly called the Hollywood principle. This is because, after an audition for a role at Hollywood, you may be told \"Don't call us, we'll call you\", meaning that they won't tell you if they chose you for the role immediately, but they'll call you in case they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "Second call\n",
      "After call\n"
     ]
    }
   ],
   "source": [
    "# Syncronous\n",
    "wait_and_print(\"First call\")\n",
    "wait_and_print(\"Second call\")\n",
    "print(\"After call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After submission\n"
     ]
    }
   ],
   "source": [
    "# Async\n",
    "wait_and_print_async(\"First call async\")\n",
    "wait_and_print_async(\"Second call async\")\n",
    "print(\"After submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synchronous version behaves in a very familiar way. The code waits for a second, prints First call, waits for another second, and then prints the Second call and After call messages. In the asynchronous version, wait_and_print_async submits (rather than execute) those calls and moves on immediately. You can see this mechanism in action by acknowledging that the \"After submission\" message is printed immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_request_async(number, on_done):\n",
    "    def timer_done():\n",
    "        on_done({\"success\": True,\n",
    "                 \"result\": number ** 2})\n",
    "    timer = threading.Timer(1.0, timer_done)\n",
    "    timer.start() \n",
    "    \n",
    "def on_done(result):\n",
    "    print(result)\n",
    "    \n",
    "network_request_async(2, on_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After submission\n"
     ]
    }
   ],
   "source": [
    "network_request_async(2, on_done)\n",
    "network_request_async(3, on_done)\n",
    "network_request_async(4, on_done)\n",
    "print(\"After submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_square(number):\n",
    "    def on_done(response):\n",
    "        if response[\"success\"]:\n",
    "            print(\"Result is: {}\".format(response[\"result\"]))\n",
    "            \n",
    "    network_request_async(number, on_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A future is an abstraction that helps us keep track of the requested resources and that we are waiting to become available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asyncio in /Users/boyuan/anaconda3/envs/Python/lib/python3.7/site-packages (3.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future pending>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "fut = asyncio.Future()\n",
    "fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut.set_result(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fut.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "fut = asyncio.Future()\n",
    "fut.add_done_callback(lambda future: print(future.result(), flush=True))\n",
    "fut.set_result(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import Future\n",
    "\n",
    "def network_request_async(number):\n",
    "    future = Future()\n",
    "    result = {\"success\": True, \"result\": number ** 2}\n",
    "    timer = threading.Timer(1.0, lambda: future.set_result(result))\n",
    "    timer.start()\n",
    "    return future\n",
    "\n",
    "fut = network_request_async(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_square(number):\n",
    "    fut = network_request_async(number)\n",
    "    \n",
    "    def on_done_future(future):\n",
    "        response = future.result()\n",
    "        if response[\"success\"]:\n",
    "            print(\"Result is: {}\".format(response[\"result\"]))\n",
    "    fut.add_done_callback(on_done_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futures are a different and slightly more convenient way of working with callbacks. Futures are also advantageous, because they can keep track of the resource status, cancel (unschedule) scheduled tasks, and handle exceptions more naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 Event loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind an event loop is to continuously monitor the status of the various resources (for example, network connections and database queries) and trigger the execution of callbacks when events take place (for example, when a resource is ready or when a timer expires)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just stick to threading? Events loops are sometimes preferred as every unit of execution never runs at the same time as another and this can simplify dealing with shared variables, data structures, and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, timeout):\n",
    "        self.timeout = timeout\n",
    "        self.start = time.time()\n",
    "    def done(self):\n",
    "        return time.time() - self.start > self.timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer is done!\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(1.0)\n",
    "\n",
    "while True:\n",
    "    if timer.done():\n",
    "        print(\"Timer is done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for events to happen by continuously polling using a loop is commonly termed as busy-waiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "# ... previous code\n",
    "\n",
    "    def on_timer_done(self, callback):\n",
    "        self.callback = callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on_timer_done merely stores a reference to the callback. The entity that monitors the event and executes the callback is the loop. This concept is demonstrated as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer(1.0)\n",
    "timer.on_timer_done(lambda: print(\"Timer is done!\"))\n",
    "\n",
    "while True:\n",
    "    if timer.done():\n",
    "        timer.callback()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timers = []\n",
    "\n",
    "timer1 = Timer(1.0)\n",
    "timer1.on_timer_done(lambda: print(\"First timer is done!\"))\n",
    "\n",
    "timer2 = Timer(2.0)\n",
    "timer2.on_timer_done(lambda: print(\"Second timer is done!\"))\n",
    "\n",
    "timers.append(timer1)\n",
    "timers.append(timer2)\n",
    "\n",
    "while True:\n",
    "    for timer in timers:\n",
    "        if timer.done():\n",
    "            timer.callback()\n",
    "            timers.remove(timer)\n",
    "    # If no more timers are left, we exit the loop\n",
    "    if len(timers) == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main restriction of an event loop is, since the flow of execution is managed by a continuously running loop, that it never uses blocking calls. If we use any blocking statement (such as time.sleep) inside the loop, you can imagine how the event monitoring and callback dispatching will stop until the blocking call is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, rather than using a blocking call, such as time.sleep, we let the event loop detect and execute the callback when the resource is ready. By not blocking the execution flow, the event loop is free to monitor multiple resources in a concurrent way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 6.2 The asyncio framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "def callback():\n",
    "    print('Hello, asyncio')\n",
    "    loop.stop()\n",
    "\n",
    "loop.call_later(1.0, callback)\n",
    "loop.run_forever()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Coroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main problems with callbacks is that they require you to break the program execution into small functions that will be invoked when a certain event takes place. As we saw in the earlier sections, callbacks can quickly become cumbersome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coroutines are another, perhaps a more natural, way to break up the program execution into chunks. They allow the programmer to write code that resembles synchronous code but will execute asynchronously. You may think of a coroutine as a function that can be stopped and resumed. A basic example of coroutines is generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators can be defined in Python using the yield statement inside a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object range_generator at 0x1120f9b50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def range_generator(n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        print('Generating value {}'.format(i))\n",
    "        yield 1\n",
    "        i += 1\n",
    "        \n",
    "generator = range_generator(3)\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of a yield statement as a breakpoint where we can stop and resume execution (while also maintaining the internal state of the generator). This ability of stopping and resuming execution can be leveraged by the event loop to allow for concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python world, a generator that can also receive values is called a generator-based coroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parrot says: Hello\n",
      "Parrot says: World\n"
     ]
    }
   ],
   "source": [
    "def parrot():\n",
    "    while True:\n",
    "        message = yield\n",
    "        print(\"Parrot says: {}\".format(message))\n",
    "        \n",
    "generator = parrot()\n",
    "generator.send(None)\n",
    "generator.send(\"Hello\")\n",
    "generator.send(\"World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object hello at 0x112149200>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def hello():\n",
    "    print('Hello, async!')\n",
    "\n",
    "coro = hello()\n",
    "coro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(coro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def wait_and_print(msg):\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"Message: \", msg)\n",
    "    \n",
    "loop.run_until_complete(wait_and_print(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def network_request(number):\n",
    "    await asyncio.sleep(1.0)\n",
    "    return {\"success\": True, \"result\": number ** 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_square(number):\n",
    "    response = await network_request(number)\n",
    "    if response[\"success\"]:\n",
    "        print(\"Result is: {}\".format(response[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.run_until_complete(fetch_square(2))\n",
    "loop.run_until_complete(fetch_square(3))\n",
    "loop.run_until_complete(fetch_square(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task pending coro=<fetch_square() running at <ipython-input-13-044634bf1e4c>:1>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asyncio.ensure_future(fetch_square(2))\n",
    "asyncio.ensure_future(fetch_square(3))\n",
    "asyncio.ensure_future(fetch_square(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.run_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Converting blocking code into non-blocking code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An effective strategy for dealing with blocking code is to run it in a separate thread. Threads are implemented at the Operating System (OS) level and allow parallel execution of blocking code. For this purpose, Python provides the Executor interface designed to run tasks in a separate thread and to monitor their progress using futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future at 0x112200d50 state=finished raised NameError>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(max_workers=3)\n",
    "\n",
    "def wait_and_return(msg):\n",
    "    time.sleep(1)\n",
    "    return msg\n",
    "\n",
    "executor.submit(wait_and_return, \"Hello. executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut = loop.run_in_executor(executor, wait_and_return, \"Hello, asyncio executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.run_until_complete(fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "async def fetch_urls(urls):\n",
    "    responses = []\n",
    "    for url in urls:\n",
    "        responses.append(await loop.run_in_executor\n",
    "                        (executor, requests.get, url))\n",
    "    return responses\n",
    "\n",
    "loop.run_until_complete(fetch_urls(['http://www.google.com',\n",
    "'http://www.example.com',\n",
    "'http://www.facebook.com']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_urls(urls):\n",
    "    return asyncio.gather(*[loop.run_in_executor\n",
    "                            (executor, requests.get, url)\n",
    "                            for url in urls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 6.3 Reactive programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reactive manifesto:\n",
    "1. **Responsive**: The system responds immediately to the user\n",
    "2. **Elastic**: The system is capable of handling different levels of load and is able to adapt to accommodate increasing demands\n",
    "3. **Resilient**: The system deals with failure gracefully. This is achieved by modularity and avoiding having a single point of failure\n",
    "4. **Message driven**: The system should not block and take advantage of events and messages. A message-driven application helps achieve all the previous requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rx import Observable\n",
    "obs = Observable.from_iterable(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.subscribe(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observables are ordered collections of items just like lists or, more generally, iterators. The term observable comes from the combination of observer and iterable. An observer is an object that reacts to changes of the variable it observes, while an iterable is an object that is capable of producing and keeping track of an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next\n",
      "1\n",
      "2\n",
      "For loop\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "collection = list([1, 2, 3, 4, 5])\n",
    "iterator = iter(collection)\n",
    "print(\"Next\")\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(\"For loop\")\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how, every time we call next or we iterate, the iterator produces a value and advances. In a sense, we are pulling results from the iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterators sound a lot like generators; however, they are more general. In Python, generators are returned by functions that use yield expressions. As we saw, generators support next, therefore, they are a special class of iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Observable.from_iter(range(4))\n",
    "obs.subscribe(on_next=lambda x: print(on_next=\"Next item: {}\"),\n",
    "              on_completed=lambda: print(\"No more data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Observable.from_iterable(range(100000))\n",
    "obs2 = obs.take(4)\n",
    "obs2.subscribe(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Useful operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Observable.from_iterable(range(4))\n",
    ".map(lambda x: x**2)\n",
    ".subscribe(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = (Observable.from_range(range(4))\n",
    ".group_by(lambda x: x % 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.subscribe(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.subscribe(lambda x: print(\"group key: \", x.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.take(1).subscribe(lambda x: x.subscribe(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.merge_all().subscribe(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_replay(a):\n",
    "    result = a.replay(None)\n",
    "    result.connect()\n",
    "    return result\n",
    "obs.map(make_replay).concat_all().subscribe(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Hot and cold observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Observable.interval(1000)\n",
    "obs.take(4).subscribe(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "obs = Observable.interval(1000).map(lambda a: (a, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "obs.take(4).subscribe(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "obs = Observable.interval(1000).map(lambda a: (a, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "obs.take(4).subscribe(lambda x: print(\"First subscriber: {}\".format(x)))\n",
    "time.sleep(0.5)\n",
    "obs.take(4).subscribe(lambda x: print(\"Second subscriber: {}\".format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "obs = Observable.interval(1000).map(lambda a: (a, time.time() - start)).publish()\n",
    "obs.take(4).subscribe(lambda x: print(\"First subscriber: {}\".format(x)))\n",
    "obs.connect() # Data production starts here\n",
    "time.sleep(2)\n",
    "obs.take(4).subscribe(lambda x: print(\"Second subscriber: {}\".format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "obs = Observable.interval(1000).map(lambda a: (a, time.time() - bstart)).replay(None)\n",
    "obs.take(4).subscribe(lambda x: print(\"First subscriber: {}\".format(x)))\n",
    "obs.connect()\n",
    "time.sleep(2)\n",
    "obs.take(4).subscribe(lambda x: print(\"Second subscriber: {}\".format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Subject()\n",
    "s.subscribe(lambda a: print(\"Subject emitted value: {}\".format(x))\n",
    "s.on_next(1)\n",
    "# Subject emitted value: 1\n",
    "s.on_next(2)\n",
    "# Subject emitted value: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Building a CPU monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_percent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rx import Observable\n",
    "cpu_data = (Observable\n",
    ".interval(100) # Each 100 milliseconds\n",
    ".map(lambda x: psutil.cpu_percent())\n",
    ".publish())\n",
    "cpu_data.connect() # Start producing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_data.take(4).subscribe(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "def monitor_cpu(npoints):\n",
    "    lines, = plt.plot([], [])\n",
    "    plt.xlim(0, npoints)\n",
    "    plt.ylim(0, 100) # 0 to 100 percent\n",
    "    cpu_data_window = cpu_data.buffer_with_count(npoints, 1)\n",
    "    def update_plot(cpu_readings):\n",
    "        lines.set_xdata(np.arange(npoints))\n",
    "        lines.set_ydata(np.array(cpu_readings))\n",
    "        plt.draw()\n",
    "cpu_data_window.subscribe(update_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alertpoints = 4\n",
    "high_cpu = (cpu_data\n",
    ".buffer_with_count(alertpoints, 1)\n",
    ".map(lambda readings: all(r > 20 for r in readings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = plt.text(1, 1, \"normal\")\n",
    "def update_warning(is_high):\n",
    "    if is_high:\n",
    "        label.set_text(\"high\")\n",
    "    else:\n",
    "        label.set_text(\"normal\")\n",
    "high_cpu.subscribe(update_warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 6.4 Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
